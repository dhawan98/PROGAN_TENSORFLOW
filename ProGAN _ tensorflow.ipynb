{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import osgeo\n",
    "import time\n",
    "import re\n",
    "import bisect\n",
    "from collections import OrderedDict\n",
    "import scipy.ndimage\n",
    "import scipy.misc\n",
    "from osgeo import gdal\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "import importlib\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:171: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:171: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-3-e3f10762ab63>:171: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if v.shape.ndims is 0:\n",
      "<ipython-input-3-e3f10762ab63>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif v.shape.ndims is 1:\n"
     ]
    }
   ],
   "source": [
    "####tfutil####\n",
    "\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "# 4.0 International License. To view a copy of this license, visit\n",
    "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Convenience.\n",
    "\n",
    "def run(*args, **kwargs): # Run the specified ops in the default session.\n",
    "    return tf.get_default_session().run(*args, **kwargs)\n",
    "\n",
    "def is_tf_expression(x):\n",
    "    return isinstance(x, tf.Tensor) or isinstance(x, tf.Variable) or isinstance(x, tf.Operation)\n",
    "\n",
    "def shape_to_list(shape):\n",
    "    return [dim.value for dim in shape]\n",
    "\n",
    "def flatten(x):\n",
    "    with tf.name_scope('Flatten'):\n",
    "        return tf.reshape(x, [-1])\n",
    "\n",
    "def log2(x):\n",
    "    with tf.name_scope('Log2'):\n",
    "        return tf.log(x) * np.float32(1.0 / np.log(2.0))\n",
    "\n",
    "def exp2(x):\n",
    "    with tf.name_scope('Exp2'):\n",
    "        return tf.exp(x * np.float32(np.log(2.0)))\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    with tf.name_scope('Lerp'):\n",
    "        return a + (b - a) * t\n",
    "\n",
    "def lerp_clip(a, b, t):\n",
    "    with tf.name_scope('LerpClip'):\n",
    "        return a + (b - a) * tf.clip_by_value(t, 0.0, 1.0)\n",
    "\n",
    "def absolute_name_scope(scope): # Forcefully enter the specified name scope, ignoring any surrounding scopes.\n",
    "    return tf.name_scope(scope + '/')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Initialize TensorFlow graph and session using good default settings.\n",
    "\n",
    "def init_tf(config_dict=dict()):\n",
    "    if tf.get_default_session() is None:\n",
    "        tf.set_random_seed(np.random.randint(1 << 31))\n",
    "        create_session(config_dict, force_as_default=True)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Create tf.Session based on config dict of the form\n",
    "# {'gpu_options.allow_growth': True}\n",
    "\n",
    "def create_session(config_dict=dict(), force_as_default=False):\n",
    "    config = tf.ConfigProto()\n",
    "    for key, value in config_dict.items():\n",
    "        fields = key.split('.')\n",
    "        obj = config\n",
    "        for field in fields[:-1]:\n",
    "            obj = getattr(obj, field)\n",
    "        setattr(obj, fields[-1], value)\n",
    "    session = tf.Session(config=config)\n",
    "    if force_as_default:\n",
    "        session._default_session = session.as_default()\n",
    "        session._default_session.enforce_nesting = False\n",
    "        session._default_session.__enter__()\n",
    "    return session\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Initialize all tf.Variables that have not already been initialized.\n",
    "# Equivalent to the following, but more efficient and does not bloat the tf graph:\n",
    "#   tf.variables_initializer(tf.report_unitialized_variables()).run()\n",
    "\n",
    "def init_uninited_vars(vars=None):\n",
    "    if vars is None: vars = tf.global_variables()\n",
    "    test_vars = []; test_ops = []\n",
    "    with tf.control_dependencies(None): # ignore surrounding control_dependencies\n",
    "        for var in vars:\n",
    "            assert is_tf_expression(var)\n",
    "            try:\n",
    "                tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/IsVariableInitialized:0'))\n",
    "            except KeyError:\n",
    "                # Op does not exist => variable may be uninitialized.\n",
    "                test_vars.append(var)\n",
    "                with absolute_name_scope(var.name.split(':')[0]):\n",
    "                    test_ops.append(tf.is_variable_initialized(var))\n",
    "    init_vars = [var for var, inited in zip(test_vars, run(test_ops)) if not inited]\n",
    "    run([var.initializer for var in init_vars])\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Set the values of given tf.Variables.\n",
    "# Equivalent to the following, but more efficient and does not bloat the tf graph:\n",
    "#   tfutil.run([tf.assign(var, value) for var, value in var_to_value_dict.items()]\n",
    "\n",
    "def set_vars(var_to_value_dict):\n",
    "    ops = []\n",
    "    feed_dict = {}\n",
    "    for var, value in var_to_value_dict.items():\n",
    "        assert is_tf_expression(var)\n",
    "        try:\n",
    "            setter = tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/setter:0')) # look for existing op\n",
    "        except KeyError:\n",
    "            with absolute_name_scope(var.name.split(':')[0]):\n",
    "                with tf.control_dependencies(None): # ignore surrounding control_dependencies\n",
    "                    setter = tf.assign(var, tf.placeholder(var.dtype, var.shape, 'new_value'), name='setter') # create new setter\n",
    "        ops.append(setter)\n",
    "        feed_dict[setter.op.inputs[1]] = value\n",
    "    run(ops, feed_dict)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Autosummary creates an identity op that internally keeps track of the input\n",
    "# values and automatically shows up in TensorBoard. The reported value\n",
    "# represents an average over input components. The average is accumulated\n",
    "# constantly over time and flushed when save_summaries() is called.\n",
    "#\n",
    "# Notes:\n",
    "# - The output tensor must be used as an input for something else in the\n",
    "#   graph. Otherwise, the autosummary op will not get executed, and the average\n",
    "#   value will not get accumulated.\n",
    "# - It is perfectly fine to include autosummaries with the same name in\n",
    "#   several places throughout the graph, even if they are executed concurrently.\n",
    "# - It is ok to also pass in a python scalar or numpy array. In this case, it\n",
    "#   is added to the average immediately.\n",
    "\n",
    "_autosummary_vars = OrderedDict() # name => [var, ...]\n",
    "_autosummary_immediate = OrderedDict() # name => update_op, update_value\n",
    "_autosummary_finalized = False\n",
    "\n",
    "def autosummary(name, value):\n",
    "    id = name.replace('/', '_')\n",
    "    if is_tf_expression(value):\n",
    "        with tf.name_scope('summary_' + id), tf.device(value.device):\n",
    "            update_op = _create_autosummary_var(name, value)\n",
    "            with tf.control_dependencies([update_op]):\n",
    "                return tf.identity(value)\n",
    "    else: # python scalar or numpy array\n",
    "        if name not in _autosummary_immediate:\n",
    "            with absolute_name_scope('Autosummary/' + id), tf.device(None), tf.control_dependencies(None):\n",
    "                update_value = tf.placeholder(tf.float32)\n",
    "                update_op = _create_autosummary_var(name, update_value)\n",
    "                _autosummary_immediate[name] = update_op, update_value\n",
    "        update_op, update_value = _autosummary_immediate[name]\n",
    "        run(update_op, {update_value: np.float32(value)})\n",
    "        return value\n",
    "\n",
    "# Create the necessary ops to include autosummaries in TensorBoard report.\n",
    "# Note: This should be done only once per graph.\n",
    "def finalize_autosummaries():\n",
    "    global _autosummary_finalized\n",
    "    if _autosummary_finalized:\n",
    "        return\n",
    "    _autosummary_finalized = True\n",
    "    init_uninited_vars([var for vars in _autosummary_vars.values() for var in vars])\n",
    "    with tf.device(None), tf.control_dependencies(None):\n",
    "        for name, vars in _autosummary_vars.items():\n",
    "            id = name.replace('/', '_')\n",
    "            with absolute_name_scope('Autosummary/' + id):\n",
    "                sum = tf.add_n(vars)\n",
    "                avg = sum[0] / sum[1]\n",
    "                with tf.control_dependencies([avg]): # read before resetting\n",
    "                    reset_ops = [tf.assign(var, tf.zeros(2)) for var in vars]\n",
    "                    with tf.name_scope(None), tf.control_dependencies(reset_ops): # reset before reporting\n",
    "                        tf.summary.scalar(name, avg)\n",
    "\n",
    "# Internal helper for creating autosummary accumulators.\n",
    "def _create_autosummary_var(name, value_expr):\n",
    "    assert not _autosummary_finalized\n",
    "    v = tf.cast(value_expr, tf.float32)\n",
    "    if v.shape.ndims is 0:\n",
    "        v = [v, np.float32(1.0)]\n",
    "    elif v.shape.ndims is 1:\n",
    "        v = [tf.reduce_sum(v), tf.cast(tf.shape(v)[0], tf.float32)]\n",
    "    else:\n",
    "        v = [tf.reduce_sum(v), tf.reduce_prod(tf.cast(tf.shape(v), tf.float32))]\n",
    "    v = tf.cond(tf.is_finite(v[0]), lambda: tf.stack(v), lambda: tf.zeros(2))\n",
    "    with tf.control_dependencies(None):\n",
    "        var = tf.Variable(tf.zeros(2)) # [numerator, denominator]\n",
    "    update_op = tf.cond(tf.is_variable_initialized(var), lambda: tf.assign_add(var, v), lambda: tf.assign(var, v))\n",
    "    if name in _autosummary_vars:\n",
    "        _autosummary_vars[name].append(var)\n",
    "    else:\n",
    "        _autosummary_vars[name] = [var]\n",
    "    return update_op\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Call filewriter.add_summary() with all summaries in the default graph,\n",
    "# automatically finalizing and merging them on the first call.\n",
    "\n",
    "_summary_merge_op = None\n",
    "\n",
    "def save_summaries(filewriter, global_step=None):\n",
    "    global _summary_merge_op\n",
    "    if _summary_merge_op is None:\n",
    "        finalize_autosummaries()\n",
    "        with tf.device(None), tf.control_dependencies(None):\n",
    "            _summary_merge_op = tf.summary.merge_all()\n",
    "    filewriter.add_summary(_summary_merge_op.eval(), global_step)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Utilities for importing modules and objects by name.\n",
    "\n",
    "def import_module(module_or_obj_name):\n",
    "    parts = module_or_obj_name.split('.')\n",
    "    parts[0] = {'np': 'numpy', 'tf': 'tensorflow'}.get(parts[0], parts[0])\n",
    "    for i in range(len(parts), 0, -1):\n",
    "        try:\n",
    "            module = importlib.import_module('.'.join(parts[:i]))\n",
    "            relative_obj_name = '.'.join(parts[i:])\n",
    "            return module, relative_obj_name\n",
    "        except ImportError:\n",
    "            pass\n",
    "    raise ImportError(module_or_obj_name)\n",
    "\n",
    "def find_obj_in_module(module, relative_obj_name):\n",
    "    obj = module\n",
    "    for part in relative_obj_name.split('.'):\n",
    "        obj = getattr(obj, part)\n",
    "    return obj\n",
    "\n",
    "def import_obj(obj_name):\n",
    "    module, relative_obj_name = import_module(obj_name)\n",
    "    return find_obj_in_module(module, relative_obj_name)\n",
    "\n",
    "def call_func_by_name(*args, func=None, **kwargs):\n",
    "    assert func is not None\n",
    "    return import_obj(func)(*args, **kwargs)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Wrapper for tf.train.Optimizer that automatically takes care of:\n",
    "# - Gradient averaging for multi-GPU training.\n",
    "# - Dynamic loss scaling and typecasts for FP16 training.\n",
    "# - Ignoring corrupted gradients that contain NaNs/Infs.\n",
    "# - Reporting statistics.\n",
    "# - Well-chosen default settings.\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name                = 'Train',\n",
    "        tf_optimizer        = 'tf.train.AdamOptimizer',\n",
    "        learning_rate       = 0.001,\n",
    "        use_loss_scaling    = False,\n",
    "        loss_scaling_init   = 64.0,\n",
    "        loss_scaling_inc    = 0.0005,\n",
    "        loss_scaling_dec    = 1.0,\n",
    "        **kwargs):\n",
    "\n",
    "        # Init fields.\n",
    "        self.name               = name\n",
    "        self.learning_rate      = tf.convert_to_tensor(learning_rate)\n",
    "        self.id                 = self.name.replace('/', '.')\n",
    "        self.scope              = tf.get_default_graph().unique_name(self.id)\n",
    "        self.optimizer_class    = import_obj(tf_optimizer)\n",
    "        self.optimizer_kwargs   = dict(kwargs)\n",
    "        self.use_loss_scaling   = use_loss_scaling\n",
    "        self.loss_scaling_init  = loss_scaling_init\n",
    "        self.loss_scaling_inc   = loss_scaling_inc\n",
    "        self.loss_scaling_dec   = loss_scaling_dec\n",
    "        self._grad_shapes       = None          # [shape, ...]\n",
    "        self._dev_opt           = OrderedDict() # device => optimizer\n",
    "        self._dev_grads         = OrderedDict() # device => [[(grad, var), ...], ...]\n",
    "        self._dev_ls_var        = OrderedDict() # device => variable (log2 of loss scaling factor)\n",
    "        self._updates_applied   = False\n",
    "\n",
    "    # Register the gradients of the given loss function with respect to the given variables.\n",
    "    # Intended to be called once per GPU.\n",
    "    def register_gradients(self, loss, vars):\n",
    "        assert not self._updates_applied\n",
    "\n",
    "        # Validate arguments.\n",
    "        if isinstance(vars, dict):\n",
    "            vars = list(vars.values()) # allow passing in Network.trainables as vars\n",
    "        assert isinstance(vars, list) and len(vars) >= 1\n",
    "        assert all(is_tf_expression(expr) for expr in vars + [loss])\n",
    "        if self._grad_shapes is None:\n",
    "            self._grad_shapes = [shape_to_list(var.shape) for var in vars]\n",
    "        assert len(vars) == len(self._grad_shapes)\n",
    "        assert all(shape_to_list(var.shape) == var_shape for var, var_shape in zip(vars, self._grad_shapes))\n",
    "        dev = loss.device\n",
    "        assert all(var.device == dev for var in vars)\n",
    "\n",
    "        # Register device and compute gradients.\n",
    "        with tf.name_scope(self.id + '_grad'), tf.device(dev):\n",
    "            if dev not in self._dev_opt:\n",
    "                opt_name = self.scope.replace('/', '_') + '_opt%d' % len(self._dev_opt)\n",
    "                self._dev_opt[dev] = self.optimizer_class(name=opt_name, learning_rate=self.learning_rate, **self.optimizer_kwargs)\n",
    "                self._dev_grads[dev] = []\n",
    "            loss = self.apply_loss_scaling(tf.cast(loss, tf.float32))\n",
    "            grads = self._dev_opt[dev].compute_gradients(loss, vars, gate_gradients=tf.train.Optimizer.GATE_NONE) # disable gating to reduce memory usage\n",
    "            grads = [(g, v) if g is not None else (tf.zeros_like(v), v) for g, v in grads] # replace disconnected gradients with zeros\n",
    "            self._dev_grads[dev].append(grads)\n",
    "\n",
    "    # Construct training op to update the registered variables based on their gradients.\n",
    "    def apply_updates(self):\n",
    "        assert not self._updates_applied\n",
    "        self._updates_applied = True\n",
    "        devices = list(self._dev_grads.keys())\n",
    "        total_grads = sum(len(grads) for grads in self._dev_grads.values())\n",
    "        assert len(devices) >= 1 and total_grads >= 1\n",
    "        ops = []\n",
    "        with absolute_name_scope(self.scope):\n",
    "\n",
    "            # Cast gradients to FP32 and calculate partial sum within each device.\n",
    "            dev_grads = OrderedDict() # device => [(grad, var), ...]\n",
    "            for dev_idx, dev in enumerate(devices):\n",
    "                with tf.name_scope('ProcessGrads%d' % dev_idx), tf.device(dev):\n",
    "                    sums = []\n",
    "                    for gv in zip(*self._dev_grads[dev]):\n",
    "                        assert all(v is gv[0][1] for g, v in gv)\n",
    "                        g = [tf.cast(g, tf.float32) for g, v in gv]\n",
    "                        g = g[0] if len(g) == 1 else tf.add_n(g)\n",
    "                        sums.append((g, gv[0][1]))\n",
    "                    dev_grads[dev] = sums\n",
    "\n",
    "            # Sum gradients across devices.\n",
    "            if len(devices) > 1:\n",
    "                with tf.name_scope('SumAcrossGPUs'), tf.device(None):\n",
    "                    for var_idx, grad_shape in enumerate(self._grad_shapes):\n",
    "                        g = [dev_grads[dev][var_idx][0] for dev in devices]\n",
    "                        if np.prod(grad_shape): # nccl does not support zero-sized tensors\n",
    "                            g = tf.contrib.nccl.all_sum(g)\n",
    "                        for dev, gg in zip(devices, g):\n",
    "                            dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])\n",
    "\n",
    "            # Apply updates separately on each device.\n",
    "            for dev_idx, (dev, grads) in enumerate(dev_grads.items()):\n",
    "                with tf.name_scope('ApplyGrads%d' % dev_idx), tf.device(dev):\n",
    "\n",
    "                    # Scale gradients as needed.\n",
    "                    if self.use_loss_scaling or total_grads > 1:\n",
    "                        with tf.name_scope('Scale'):\n",
    "                            coef = tf.constant(np.float32(1.0 / total_grads), name='coef')\n",
    "                            coef = self.undo_loss_scaling(coef)\n",
    "                            grads = [(g * coef, v) for g, v in grads]\n",
    "\n",
    "                    # Check for overflows.\n",
    "                    with tf.name_scope('CheckOverflow'):\n",
    "                        grad_ok = tf.reduce_all(tf.stack([tf.reduce_all(tf.is_finite(g)) for g, v in grads]))\n",
    "\n",
    "                    # Update weights and adjust loss scaling.\n",
    "                    with tf.name_scope('UpdateWeights'):\n",
    "                        opt = self._dev_opt[dev]\n",
    "                        ls_var = self.get_loss_scaling_var(dev)\n",
    "                        if not self.use_loss_scaling:\n",
    "                            ops.append(tf.cond(grad_ok, lambda: opt.apply_gradients(grads), tf.no_op))\n",
    "                        else:\n",
    "                            ops.append(tf.cond(grad_ok,\n",
    "                                lambda: tf.group(tf.assign_add(ls_var, self.loss_scaling_inc), opt.apply_gradients(grads)),\n",
    "                                lambda: tf.group(tf.assign_sub(ls_var, self.loss_scaling_dec))))\n",
    "\n",
    "                    # Report statistics on the last device.\n",
    "                    if dev == devices[-1]:\n",
    "                        with tf.name_scope('Statistics'):\n",
    "                            ops.append(autosummary(self.id + '/learning_rate', self.learning_rate))\n",
    "                            ops.append(autosummary(self.id + '/overflow_frequency', tf.where(grad_ok, 0, 1)))\n",
    "                            if self.use_loss_scaling:\n",
    "                                ops.append(autosummary(self.id + '/loss_scaling_log2', ls_var))\n",
    "\n",
    "            # Initialize variables and group everything into a single op.\n",
    "            self.reset_optimizer_state()\n",
    "            init_uninited_vars(list(self._dev_ls_var.values()))\n",
    "            return tf.group(*ops, name='TrainingOp')\n",
    "\n",
    "    # Reset internal state of the underlying optimizer.\n",
    "    def reset_optimizer_state(self):\n",
    "        run([var.initializer for opt in self._dev_opt.values() for var in opt.variables()])\n",
    "\n",
    "    # Get or create variable representing log2 of the current dynamic loss scaling factor.\n",
    "    def get_loss_scaling_var(self, device):\n",
    "        if not self.use_loss_scaling:\n",
    "            return None\n",
    "        if device not in self._dev_ls_var:\n",
    "            with absolute_name_scope(self.scope + '/LossScalingVars'), tf.control_dependencies(None):\n",
    "                self._dev_ls_var[device] = tf.Variable(np.float32(self.loss_scaling_init), name='loss_scaling_var')\n",
    "        return self._dev_ls_var[device]\n",
    "\n",
    "    # Apply dynamic loss scaling for the given expression.\n",
    "    def apply_loss_scaling(self, value):\n",
    "        assert is_tf_expression(value)\n",
    "        if not self.use_loss_scaling:\n",
    "            return value\n",
    "        return value * exp2(self.get_loss_scaling_var(value.device))\n",
    "\n",
    "    # Undo the effect of dynamic loss scaling for the given expression.\n",
    "    def undo_loss_scaling(self, value):\n",
    "        assert is_tf_expression(value)\n",
    "        if not self.use_loss_scaling:\n",
    "            return value\n",
    "        return value * exp2(-self.get_loss_scaling_var(value.device))\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Generic network abstraction.\n",
    "#\n",
    "# Acts as a convenience wrapper for a parameterized network construction\n",
    "# function, providing several utility methods and convenient access to\n",
    "# the inputs/outputs/weights.\n",
    "#\n",
    "# Network objects can be safely pickled and unpickled for long-term\n",
    "# archival purposes. The pickling works reliably as long as the underlying\n",
    "# network construction function is defined in a standalone Python module\n",
    "# that has no side effects or application-specific imports.\n",
    "\n",
    "network_import_handlers = []    # Custom import handlers for dealing with legacy data in pickle import.\n",
    "_network_import_modules = []    # Temporary modules create during pickle import.\n",
    "\n",
    "class Network:\n",
    "    def __init__(self,\n",
    "        name=None,          # Network name. Used to select TensorFlow name and variable scopes.\n",
    "        func=None,          # Fully qualified name of the underlying network construction function.\n",
    "        **static_kwargs):   # Keyword arguments to be passed in to the network construction function.\n",
    "\n",
    "        self._init_fields()\n",
    "        self.name = name\n",
    "        self.static_kwargs = dict(static_kwargs)\n",
    "\n",
    "        # Init build func.\n",
    "        module, self._build_func_name = import_module(func)\n",
    "        self._build_module_src = inspect.getsource(module)\n",
    "        self._build_func = find_obj_in_module(module, self._build_func_name)\n",
    "\n",
    "        # Init graph.\n",
    "        self._init_graph()\n",
    "        self.reset_vars()\n",
    "\n",
    "    def _init_fields(self):\n",
    "        self.name               = None          # User-specified name, defaults to build func name if None.\n",
    "        self.scope              = None          # Unique TF graph scope, derived from the user-specified name.\n",
    "        self.static_kwargs      = dict()        # Arguments passed to the user-supplied build func.\n",
    "        self.num_inputs         = 0             # Number of input tensors.\n",
    "        self.num_outputs        = 0             # Number of output tensors.\n",
    "        self.input_shapes       = [[]]          # Input tensor shapes (NC or NCHW), including minibatch dimension.\n",
    "        self.output_shapes      = [[]]          # Output tensor shapes (NC or NCHW), including minibatch dimension.\n",
    "        self.input_shape        = []            # Short-hand for input_shapes[0].\n",
    "        self.output_shape       = []            # Short-hand for output_shapes[0].\n",
    "        self.input_templates    = []            # Input placeholders in the template graph.\n",
    "        self.output_templates   = []            # Output tensors in the template graph.\n",
    "        self.input_names        = []            # Name string for each input.\n",
    "        self.output_names       = []            # Name string for each output.\n",
    "        self.vars               = OrderedDict() # All variables (localname => var).\n",
    "        self.trainables         = OrderedDict() # Trainable variables (localname => var).\n",
    "        self._build_func        = None          # User-supplied build function that constructs the network.\n",
    "        self._build_func_name   = None          # Name of the build function.\n",
    "        self._build_module_src  = None          # Full source code of the module containing the build function.\n",
    "        self._run_cache         = dict()        # Cached graph data for Network.run().\n",
    "        \n",
    "    def _init_graph(self):\n",
    "        # Collect inputs.\n",
    "        self.input_names = []\n",
    "        for param in inspect.signature(self._build_func).parameters.values():\n",
    "            if param.kind == param.POSITIONAL_OR_KEYWORD and param.default is param.empty:\n",
    "                self.input_names.append(param.name)\n",
    "        self.num_inputs = len(self.input_names)\n",
    "        assert self.num_inputs >= 1\n",
    "\n",
    "        # Choose name and scope.\n",
    "        if self.name is None:\n",
    "            self.name = self._build_func_name\n",
    "        self.scope = tf.get_default_graph().unique_name(self.name.replace('/', '_'), mark_as_used=False)\n",
    "        \n",
    "        # Build template graph.\n",
    "        with tf.variable_scope(self.scope, reuse=tf.AUTO_REUSE):\n",
    "            assert tf.get_variable_scope().name == self.scope\n",
    "            with absolute_name_scope(self.scope): # ignore surrounding name_scope\n",
    "                with tf.control_dependencies(None): # ignore surrounding control_dependencies\n",
    "                    self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]\n",
    "                    out_expr = self._build_func(*self.input_templates, is_template_graph=True, **self.static_kwargs)\n",
    "            \n",
    "        # Collect outputs.\n",
    "        assert is_tf_expression(out_expr) or isinstance(out_expr, tuple)\n",
    "        self.output_templates = [out_expr] if is_tf_expression(out_expr) else list(out_expr)\n",
    "        self.output_names = [t.name.split('/')[-1].split(':')[0] for t in self.output_templates]\n",
    "        self.num_outputs = len(self.output_templates)\n",
    "        assert self.num_outputs >= 1\n",
    "        \n",
    "        # Populate remaining fields.\n",
    "        self.input_shapes   = [shape_to_list(t.shape) for t in self.input_templates]\n",
    "        self.output_shapes  = [shape_to_list(t.shape) for t in self.output_templates]\n",
    "        self.input_shape    = self.input_shapes[0]\n",
    "        self.output_shape   = self.output_shapes[0]\n",
    "        self.vars           = OrderedDict([(self.get_var_localname(var), var) for var in tf.global_variables(self.scope + '/')])\n",
    "        self.trainables     = OrderedDict([(self.get_var_localname(var), var) for var in tf.trainable_variables(self.scope + '/')])\n",
    "\n",
    "    # Run initializers for all variables defined by this network.\n",
    "    def reset_vars(self):\n",
    "        run([var.initializer for var in self.vars.values()])\n",
    "\n",
    "    # Run initializers for all trainable variables defined by this network.\n",
    "    def reset_trainables(self):\n",
    "        run([var.initializer for var in self.trainables.values()])\n",
    "\n",
    "    # Get TensorFlow expression(s) for the output(s) of this network, given the inputs.\n",
    "    def get_output_for(self, *in_expr, return_as_list=False, **dynamic_kwargs):\n",
    "        assert len(in_expr) == self.num_inputs\n",
    "        all_kwargs = dict(self.static_kwargs)\n",
    "        all_kwargs.update(dynamic_kwargs)\n",
    "        with tf.variable_scope(self.scope, reuse=True):\n",
    "            assert tf.get_variable_scope().name == self.scope\n",
    "            named_inputs = [tf.identity(expr, name=name) for expr, name in zip(in_expr, self.input_names)]\n",
    "            out_expr = self._build_func(*named_inputs, **all_kwargs)\n",
    "        assert is_tf_expression(out_expr) or isinstance(out_expr, tuple)\n",
    "        if return_as_list:\n",
    "            out_expr = [out_expr] if is_tf_expression(out_expr) else list(out_expr)\n",
    "        return out_expr\n",
    "\n",
    "    # Get the local name of a given variable, excluding any surrounding name scopes.\n",
    "    def get_var_localname(self, var_or_globalname):\n",
    "        assert is_tf_expression(var_or_globalname) or isinstance(var_or_globalname, str)\n",
    "        globalname = var_or_globalname if isinstance(var_or_globalname, str) else var_or_globalname.name\n",
    "        assert globalname.startswith(self.scope + '/')\n",
    "        localname = globalname[len(self.scope) + 1:]\n",
    "        localname = localname.split(':')[0]\n",
    "        return localname\n",
    "\n",
    "    # Find variable by local or global name.\n",
    "    def find_var(self, var_or_localname):\n",
    "        assert is_tf_expression(var_or_localname) or isinstance(var_or_localname, str)\n",
    "        return self.vars[var_or_localname] if isinstance(var_or_localname, str) else var_or_localname\n",
    "\n",
    "    # Get the value of a given variable as NumPy array.\n",
    "    # Note: This method is very inefficient -- prefer to use tfutil.run(list_of_vars) whenever possible.\n",
    "    def get_var(self, var_or_localname):\n",
    "        return self.find_var(var_or_localname).eval()\n",
    "        \n",
    "    # Set the value of a given variable based on the given NumPy array.\n",
    "    # Note: This method is very inefficient -- prefer to use tfutil.set_vars() whenever possible.\n",
    "    def set_var(self, var_or_localname, new_value):\n",
    "        return set_vars({self.find_var(var_or_localname): new_value})\n",
    "\n",
    "    # Pickle export.\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'version':          2,\n",
    "            'name':             self.name,\n",
    "            'static_kwargs':    self.static_kwargs,\n",
    "            'build_module_src': self._build_module_src,\n",
    "            'build_func_name':  self._build_func_name,\n",
    "            'variables':        list(zip(self.vars.keys(), run(list(self.vars.values()))))}\n",
    "\n",
    "    # Pickle import.\n",
    "    def __setstate__(self, state):\n",
    "        self._init_fields()\n",
    "\n",
    "        # Execute custom import handlers.\n",
    "        for handler in network_import_handlers:\n",
    "            state = handler(state)\n",
    "\n",
    "        # Set basic fields.\n",
    "        assert state['version'] == 2\n",
    "        self.name = state['name']\n",
    "        self.static_kwargs = state['static_kwargs']\n",
    "        self._build_module_src = state['build_module_src']\n",
    "        self._build_func_name = state['build_func_name']\n",
    "        \n",
    "        # Parse imported module.\n",
    "        module = imp.new_module('_tfutil_network_import_module_%d' % len(_network_import_modules))\n",
    "        exec(self._build_module_src, module.__dict__)\n",
    "        self._build_func = find_obj_in_module(module, self._build_func_name)\n",
    "        _network_import_modules.append(module) # avoid gc\n",
    "        \n",
    "        # Init graph.\n",
    "        self._init_graph()\n",
    "        self.reset_vars()\n",
    "        set_vars({self.find_var(name): value for name, value in state['variables']})\n",
    "\n",
    "    # Create a clone of this network with its own copy of the variables.\n",
    "    def clone(self, name=None):\n",
    "        net = object.__new__(Network)\n",
    "        net._init_fields()\n",
    "        net.name = name if name is not None else self.name\n",
    "        net.static_kwargs = dict(self.static_kwargs)\n",
    "        net._build_module_src = self._build_module_src\n",
    "        net._build_func_name = self._build_func_name\n",
    "        net._build_func = self._build_func\n",
    "        net._init_graph()\n",
    "        net.copy_vars_from(self)\n",
    "        return net\n",
    "\n",
    "    # Copy the values of all variables from the given network.\n",
    "    def copy_vars_from(self, src_net):\n",
    "        assert isinstance(src_net, Network)\n",
    "        name_to_value = run({name: src_net.find_var(name) for name in self.vars.keys()})\n",
    "        set_vars({self.find_var(name): value for name, value in name_to_value.items()})\n",
    "\n",
    "    # Copy the values of all trainable variables from the given network.\n",
    "    def copy_trainables_from(self, src_net):\n",
    "        assert isinstance(src_net, Network)\n",
    "        name_to_value = run({name: src_net.find_var(name) for name in self.trainables.keys()})\n",
    "        set_vars({self.find_var(name): value for name, value in name_to_value.items()})\n",
    "\n",
    "    # Create new network with the given parameters, and copy all variables from this network.\n",
    "    def convert(self, name=None, func=None, **static_kwargs):\n",
    "        net = Network(name, func, **static_kwargs)\n",
    "        net.copy_vars_from(self)\n",
    "        return net\n",
    "\n",
    "    # Construct a TensorFlow op that updates the variables of this network\n",
    "    # to be slightly closer to those of the given network.\n",
    "    def setup_as_moving_average_of(self, src_net, beta=0.99, beta_nontrainable=0.0):\n",
    "        assert isinstance(src_net, Network)\n",
    "        with absolute_name_scope(self.scope):\n",
    "            with tf.name_scope('MovingAvg'):\n",
    "                ops = []\n",
    "                for name, var in self.vars.items():\n",
    "                    if name in src_net.vars:\n",
    "                        cur_beta = beta if name in self.trainables else beta_nontrainable\n",
    "                        new_value = lerp(src_net.vars[name], var, cur_beta)\n",
    "                        ops.append(var.assign(new_value))\n",
    "                return tf.group(*ops)\n",
    "\n",
    "    # Run this network for the given NumPy array(s), and return the output(s) as NumPy array(s).\n",
    "    def run(self, *in_arrays,\n",
    "        return_as_list  = False,    # True = return a list of NumPy arrays, False = return a single NumPy array, or a tuple if there are multiple outputs.\n",
    "        print_progress  = False,    # Print progress to the console? Useful for very large input arrays.\n",
    "        minibatch_size  = None,     # Maximum minibatch size to use, None = disable batching.\n",
    "        num_gpus        = 1,        # Number of GPUs to use.\n",
    "        out_mul         = 1.0,      # Multiplicative constant to apply to the output(s).\n",
    "        out_add         = 0.0,      # Additive constant to apply to the output(s).\n",
    "        out_shrink      = 1,        # Shrink the spatial dimensions of the output(s) by the given factor.\n",
    "        out_dtype       = None,     # Convert the output to the specified data type.\n",
    "        **dynamic_kwargs):          # Additional keyword arguments to pass into the network construction function.\n",
    "\n",
    "        assert len(in_arrays) == self.num_inputs\n",
    "        num_items = in_arrays[0].shape[0]\n",
    "        if minibatch_size is None:\n",
    "            minibatch_size = num_items\n",
    "        key = str([list(sorted(dynamic_kwargs.items())), num_gpus, out_mul, out_add, out_shrink, out_dtype])\n",
    "\n",
    "        # Build graph.\n",
    "        if key not in self._run_cache:\n",
    "            with absolute_name_scope(self.scope + '/Run'), tf.control_dependencies(None):\n",
    "                in_split = list(zip(*[tf.split(x, num_gpus) for x in self.input_templates]))\n",
    "                out_split = []\n",
    "                for gpu in range(num_gpus):\n",
    "                    with tf.device('/gpu:%d' % gpu):\n",
    "                        out_expr = self.get_output_for(*in_split[gpu], return_as_list=True, **dynamic_kwargs)\n",
    "                        if out_mul != 1.0:\n",
    "                            out_expr = [x * out_mul for x in out_expr]\n",
    "                        if out_add != 0.0:\n",
    "                            out_expr = [x + out_add for x in out_expr]\n",
    "                        if out_shrink > 1:\n",
    "                            ksize = [1, 1, out_shrink, out_shrink]\n",
    "                            out_expr = [tf.nn.avg_pool(x, ksize=ksize, strides=ksize, padding='VALID', data_format='NCHW') for x in out_expr]\n",
    "                        if out_dtype is not None:\n",
    "                            if tf.as_dtype(out_dtype).is_integer:\n",
    "                                out_expr = [tf.round(x) for x in out_expr]\n",
    "                            out_expr = [tf.saturate_cast(x, out_dtype) for x in out_expr]\n",
    "                        out_split.append(out_expr)\n",
    "                self._run_cache[key] = [tf.concat(outputs, axis=0) for outputs in zip(*out_split)]\n",
    "\n",
    "        # Run minibatches.\n",
    "        out_expr = self._run_cache[key]\n",
    "        out_arrays = [np.empty([num_items] + shape_to_list(expr.shape)[1:], expr.dtype.name) for expr in out_expr]\n",
    "        for mb_begin in range(0, num_items, minibatch_size):\n",
    "            if print_progress:\n",
    "                print('\\r%d / %d' % (mb_begin, num_items), end='')\n",
    "            mb_end = min(mb_begin + minibatch_size, num_items)\n",
    "            mb_in = [src[mb_begin : mb_end] for src in in_arrays]\n",
    "            mb_out = tf.get_default_session().run(out_expr, dict(zip(self.input_templates, mb_in)))\n",
    "            for dst, src in zip(out_arrays, mb_out):\n",
    "                dst[mb_begin : mb_end] = src\n",
    "\n",
    "        # Done.\n",
    "        if print_progress:\n",
    "            print('\\r%d / %d' % (num_items, num_items))\n",
    "        if not return_as_list:\n",
    "            out_arrays = out_arrays[0] if len(out_arrays) == 1 else tuple(out_arrays)\n",
    "        return out_arrays\n",
    "\n",
    "    # Returns a list of (name, output_expr, trainable_vars) tuples corresponding to\n",
    "    # individual layers of the network. Mainly intended to be used for reporting.\n",
    "    def list_layers(self):\n",
    "        patterns_to_ignore = ['/Setter', '/new_value', '/Shape', '/strided_slice', '/Cast', '/concat']\n",
    "        all_ops = tf.get_default_graph().get_operations()\n",
    "        all_ops = [op for op in all_ops if not any(p in op.name for p in patterns_to_ignore)]\n",
    "        layers = []\n",
    "\n",
    "        def recurse(scope, parent_ops, level):\n",
    "            prefix = scope + '/'\n",
    "            ops = [op for op in parent_ops if op.name == scope or op.name.startswith(prefix)]\n",
    "\n",
    "            # Does not contain leaf nodes => expand immediate children.\n",
    "            if level == 0 or all('/' in op.name[len(prefix):] for op in ops):\n",
    "                visited = set()\n",
    "                for op in ops:\n",
    "                    suffix = op.name[len(prefix):]\n",
    "                    if '/' in suffix:\n",
    "                        suffix = suffix[:suffix.index('/')]\n",
    "                    if suffix not in visited:\n",
    "                        recurse(prefix + suffix, ops, level + 1)\n",
    "                        visited.add(suffix)\n",
    "\n",
    "            # Otherwise => interpret as a layer.\n",
    "            else:\n",
    "                layer_name = scope[len(self.scope)+1:]\n",
    "                layer_output = ops[-1].outputs[0]\n",
    "                layer_trainables = [op.outputs[0] for op in ops if op.type.startswith('Variable') and self.get_var_localname(op.name) in self.trainables]\n",
    "                layers.append((layer_name, layer_output, layer_trainables))\n",
    "\n",
    "        recurse(self.scope, all_ops, 0)\n",
    "        return layers\n",
    "\n",
    "    # Print a summary table of the network structure.\n",
    "    def print_layers(self, title=None, hide_layers_with_no_params=False):\n",
    "        if title is None: title = self.name\n",
    "        print()\n",
    "        print('%-28s%-12s%-24s%-24s' % (title, 'Params', 'OutputShape', 'WeightShape'))\n",
    "        print('%-28s%-12s%-24s%-24s' % (('---',) * 4))\n",
    "\n",
    "        total_params = 0\n",
    "        for layer_name, layer_output, layer_trainables in self.list_layers():\n",
    "            weights = [var for var in layer_trainables if var.name.endswith('/weight:0')]\n",
    "            num_params = sum(np.prod(shape_to_list(var.shape)) for var in layer_trainables)\n",
    "            total_params += num_params\n",
    "            if hide_layers_with_no_params and num_params == 0:\n",
    "                continue\n",
    "\n",
    "            print('%-28s%-12s%-24s%-24s' % (\n",
    "                layer_name,\n",
    "                num_params if num_params else '-',\n",
    "                layer_output.shape,\n",
    "                weights[0].shape if len(weights) == 1 else '-'))\n",
    "\n",
    "        print('%-28s%-12s%-24s%-24s' % (('---',) * 4))\n",
    "        print('%-28s%-12s%-24s%-24s' % ('Total', total_params, '', ''))\n",
    "        print()\n",
    "\n",
    "    # Construct summary ops to include histograms of all trainable parameters in TensorBoard.\n",
    "    def setup_weight_histograms(self, title=None):\n",
    "        if title is None: title = self.name\n",
    "        with tf.name_scope(None), tf.device(None), tf.control_dependencies(None):\n",
    "            for localname, var in self.trainables.items():\n",
    "                if '/' in localname:\n",
    "                    p = localname.split('/')\n",
    "                    name = title + '_' + p[-1] + '/' + '_'.join(p[:-1])\n",
    "                else:\n",
    "                    name = title + '_toplevel/' + localname\n",
    "                tf.summary.histogram(name, var)\n",
    "\n",
    "#----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# Parse individual image from a tfrecords file.\n",
    "\n",
    "def parse_tfrecord_tf(record):\n",
    "    features = tf.parse_single_example(record, features={\n",
    "        'shape': tf.FixedLenFeature([3], tf.int64),\n",
    "        'data': tf.FixedLenFeature([], tf.string)})\n",
    "    data = tf.decode_raw(features['data'], tf.uint16)\n",
    "    return tf.reshape(data, features['shape'])\n",
    "\n",
    "def parse_tfrecord_np(record):\n",
    "    ex = tf.train.Example()\n",
    "    ex.ParseFromString(record)\n",
    "    shape = ex.features.feature['shape'].int64_list.value\n",
    "    data = ex.features.feature['data'].bytes_list.value[0]\n",
    "    return np.fromstring(data, np.uint16).reshape(shape)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Dataset class that loads data from tfrecords files.\n",
    "\n",
    "class TFRecordDataset:\n",
    "    def __init__(self,\n",
    "        tfrecord_dir,               # Directory containing a collection of tfrecords files.\n",
    "        resolution      = None,     # Dataset resolution, None = autodetect.\n",
    "        label_file      = None,     # Relative path of the labels file, None = autodetect.\n",
    "        max_label_size  = 0,        # 0 = no labels, 'full' = full labels, <int> = N first label components.\n",
    "        repeat          = True,     # Repeat dataset indefinitely.\n",
    "        shuffle_mb      = 4096,     # Shuffle data within specified window (megabytes), 0 = disable shuffling.\n",
    "        prefetch_mb     = 2048,     # Amount of data to prefetch (megabytes), 0 = disable prefetching.\n",
    "        buffer_mb       = 256,      # Read buffer size (megabytes).\n",
    "        num_threads     = 2):       # Number of concurrent threads.\n",
    "\n",
    "        self.tfrecord_dir       = tfrecord_dir\n",
    "        self.resolution         = None\n",
    "        self.resolution_log2    = None\n",
    "        self.shape              = []        # [channel, height, width]\n",
    "        self.dtype              = 'uint16'\n",
    "        self.dynamic_range      = [0, 4096]\n",
    "        self.label_file         = label_file\n",
    "        self.label_size         = None      # [component]\n",
    "        self.label_dtype        = None\n",
    "        self._np_labels         = None\n",
    "        self._tf_minibatch_in   = None\n",
    "        self._tf_labels_var     = None\n",
    "        self._tf_labels_dataset = None\n",
    "        self._tf_datasets       = dict()\n",
    "        self._tf_iterator       = None\n",
    "        self._tf_init_ops       = dict()\n",
    "        self._tf_minibatch_np   = None\n",
    "        self._cur_minibatch     = -1\n",
    "        self._cur_lod           = -1\n",
    "\n",
    "        # List tfrecords files and inspect their shapes.\n",
    "        assert os.path.isdir(self.tfrecord_dir)\n",
    "        tfr_files = sorted(glob.glob(os.path.join(self.tfrecord_dir, '*.tfrecords')))\n",
    "        assert len(tfr_files) >= 1\n",
    "        tfr_shapes = []\n",
    "        for tfr_file in tfr_files:\n",
    "            tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n",
    "            for record in tf.python_io.tf_record_iterator(tfr_file, tfr_opt):\n",
    "                tfr_shapes.append(parse_tfrecord_np(record).shape)\n",
    "                break\n",
    "\n",
    "        # Autodetect label filename.\n",
    "        if self.label_file is None:\n",
    "            guess = sorted(glob.glob(os.path.join(self.tfrecord_dir, '*.labels')))\n",
    "            if len(guess):\n",
    "                self.label_file = guess[0]\n",
    "        elif not os.path.isfile(self.label_file):\n",
    "            guess = os.path.join(self.tfrecord_dir, self.label_file)\n",
    "            if os.path.isfile(guess):\n",
    "                self.label_file = guess\n",
    "\n",
    "        # Determine shape and resolution.\n",
    "        max_shape = max(tfr_shapes, key=lambda shape: np.prod(shape))\n",
    "        self.resolution = resolution if resolution is not None else max_shape[1]\n",
    "        self.resolution_log2 = int(np.log2(self.resolution))\n",
    "        self.shape = [max_shape[0], self.resolution, self.resolution]\n",
    "        tfr_lods = [self.resolution_log2 - int(np.log2(shape[1])) for shape in tfr_shapes]\n",
    "        assert all(shape[0] == max_shape[0] for shape in tfr_shapes)\n",
    "        assert all(shape[1] == shape[2] for shape in tfr_shapes)\n",
    "        assert all(shape[1] == self.resolution // (2**lod) for shape, lod in zip(tfr_shapes, tfr_lods))\n",
    "        assert all(lod in tfr_lods for lod in range(self.resolution_log2 - 1))\n",
    "\n",
    "        # Load labels.\n",
    "        assert max_label_size == 'full' or max_label_size >= 0\n",
    "        self._np_labels = np.zeros([1<<20, 0], dtype=np.float32)\n",
    "        if self.label_file is not None and max_label_size != 0:\n",
    "            self._np_labels = np.load(self.label_file)\n",
    "            assert self._np_labels.ndim == 2\n",
    "        if max_label_size != 'full' and self._np_labels.shape[1] > max_label_size:\n",
    "            self._np_labels = self._np_labels[:, :max_label_size]\n",
    "        self.label_size = self._np_labels.shape[1]\n",
    "        self.label_dtype = self._np_labels.dtype.name\n",
    "\n",
    "        # Build TF expressions.\n",
    "        with tf.name_scope('Dataset'), tf.device('/cpu:0'):\n",
    "            self._tf_minibatch_in = tf.placeholder(tf.int64, name='minibatch_in', shape=[])\n",
    "            tf_labels_init = tf.zeros(self._np_labels.shape, self._np_labels.dtype)\n",
    "            self._tf_labels_var = tf.Variable(tf_labels_init, name='labels_var')\n",
    "            tfutil.set_vars({self._tf_labels_var: self._np_labels})\n",
    "            self._tf_labels_dataset = tf.data.Dataset.from_tensor_slices(self._tf_labels_var)\n",
    "            for tfr_file, tfr_shape, tfr_lod in zip(tfr_files, tfr_shapes, tfr_lods):\n",
    "                if tfr_lod < 0:\n",
    "                    continue\n",
    "                dset = tf.data.TFRecordDataset(tfr_file, compression_type='', buffer_size=buffer_mb<<20)\n",
    "                dset = dset.map(parse_tfrecord_tf, num_parallel_calls=num_threads)\n",
    "                dset = tf.data.Dataset.zip((dset, self._tf_labels_dataset))\n",
    "                bytes_per_item = np.prod(tfr_shape) * np.dtype(self.dtype).itemsize\n",
    "                if shuffle_mb > 0:\n",
    "                    dset = dset.shuffle(((shuffle_mb << 20) - 1) // bytes_per_item + 1)\n",
    "                if repeat:\n",
    "                    dset = dset.repeat()\n",
    "                if prefetch_mb > 0:\n",
    "                    dset = dset.prefetch(((prefetch_mb << 20) - 1) // bytes_per_item + 1)\n",
    "                dset = dset.batch(self._tf_minibatch_in)\n",
    "                self._tf_datasets[tfr_lod] = dset\n",
    "            self._tf_iterator = tf.data.Iterator.from_structure(self._tf_datasets[0].output_types, self._tf_datasets[0].output_shapes)\n",
    "            self._tf_init_ops = {lod: self._tf_iterator.make_initializer(dset) for lod, dset in self._tf_datasets.items()}\n",
    "\n",
    "    # Use the given minibatch size and level-of-detail for the data returned by get_minibatch_tf().\n",
    "    def configure(self, minibatch_size, lod=0):\n",
    "        lod = int(np.floor(lod))\n",
    "        assert minibatch_size >= 1 and lod in self._tf_datasets\n",
    "        if self._cur_minibatch != minibatch_size or self._cur_lod != lod:\n",
    "            self._tf_init_ops[lod].run({self._tf_minibatch_in: minibatch_size})\n",
    "            self._cur_minibatch = minibatch_size\n",
    "            self._cur_lod = lod\n",
    "\n",
    "    # Get next minibatch as TensorFlow expressions.\n",
    "    def get_minibatch_tf(self): # => images, labels\n",
    "        return self._tf_iterator.get_next()\n",
    "\n",
    "    # Get next minibatch as NumPy arrays.\n",
    "    def get_minibatch_np(self, minibatch_size, lod=0): # => images, labels\n",
    "        self.configure(minibatch_size, lod)\n",
    "        if self._tf_minibatch_np is None:\n",
    "            self._tf_minibatch_np = self.get_minibatch_tf()\n",
    "        return tfutil.run(self._tf_minibatch_np)\n",
    "\n",
    "    # Get random labels as TensorFlow expression.\n",
    "    def get_random_labels_tf(self, minibatch_size): # => labels\n",
    "        if self.label_size > 0:\n",
    "            return tf.gather(self._tf_labels_var, tf.random_uniform([minibatch_size], 0, self._np_labels.shape[0], dtype=tf.int32))\n",
    "        else:\n",
    "            return tf.zeros([minibatch_size, 0], self.label_dtype)\n",
    "\n",
    "    # Get random labels as NumPy array.\n",
    "    def get_random_labels_np(self, minibatch_size): # => labels\n",
    "        if self.label_size > 0:\n",
    "            return self._np_labels[np.random.randint(self._np_labels.shape[0], size=[minibatch_size])]\n",
    "        else:\n",
    "            return np.zeros([minibatch_size, 0], self.label_dtype)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Base class for datasets that are generated on the fly.\n",
    "\n",
    "class SyntheticDataset:\n",
    "    def __init__(self, resolution=1024, num_channels=13, dtype='uint16', dynamic_range=[0,4096], label_size=0, label_dtype='float32'):\n",
    "        self.resolution         = resolution\n",
    "        self.resolution_log2    = int(np.log2(resolution))\n",
    "        self.shape              = [num_channels, resolution, resolution]\n",
    "        self.dtype              = dtype\n",
    "        self.dynamic_range      = dynamic_range\n",
    "        self.label_size         = label_size\n",
    "        self.label_dtype        = label_dtype\n",
    "        self._tf_minibatch_var  = None\n",
    "        self._tf_lod_var        = None\n",
    "        self._tf_minibatch_np   = None\n",
    "        self._tf_labels_np      = None\n",
    "\n",
    "        assert self.resolution == 2 ** self.resolution_log2\n",
    "        with tf.name_scope('Dataset'):\n",
    "            self._tf_minibatch_var = tf.Variable(np.int32(0), name='minibatch_var')\n",
    "            self._tf_lod_var = tf.Variable(np.int32(0), name='lod_var')\n",
    "\n",
    "    def configure(self, minibatch_size, lod=0):\n",
    "        lod = int(np.floor(lod))\n",
    "        assert minibatch_size >= 1 and lod >= 0 and lod <= self.resolution_log2\n",
    "        tfutil.set_vars({self._tf_minibatch_var: minibatch_size, self._tf_lod_var: lod})\n",
    "\n",
    "    def get_minibatch_tf(self): # => images, labels\n",
    "        with tf.name_scope('SyntheticDataset'):\n",
    "            shrink = tf.cast(2.0 ** tf.cast(self._tf_lod_var, tf.float32), tf.int32)\n",
    "            shape = [self.shape[0], self.shape[1] // shrink, self.shape[2] // shrink]\n",
    "            images = self._generate_images(self._tf_minibatch_var, self._tf_lod_var, shape)\n",
    "            labels = self._generate_labels(self._tf_minibatch_var)\n",
    "            return images, labels\n",
    "\n",
    "    def get_minibatch_np(self, minibatch_size, lod=0): # => images, labels\n",
    "        self.configure(minibatch_size, lod)\n",
    "        if self._tf_minibatch_np is None:\n",
    "            self._tf_minibatch_np = self.get_minibatch_tf()\n",
    "        return tfutil.run(self._tf_minibatch_np)\n",
    "\n",
    "    def get_random_labels_tf(self, minibatch_size): # => labels\n",
    "        with tf.name_scope('SyntheticDataset'):\n",
    "            return self._generate_labels(minibatch_size)\n",
    "\n",
    "    def get_random_labels_np(self, minibatch_size): # => labels\n",
    "        self.configure(minibatch_size)\n",
    "        if self._tf_labels_np is None:\n",
    "            self._tf_labels_np = self.get_random_labels_tf()\n",
    "        return tfutil.run(self._tf_labels_np)\n",
    "\n",
    "    def _generate_images(self, minibatch, lod, shape): # to be overridden by subclasses\n",
    "        return tf.zeros([minibatch] + shape, self.dtype)\n",
    "\n",
    "    def _generate_labels(self, minibatch): # to be overridden by subclasses\n",
    "        return tf.zeros([minibatch, self.label_size], self.label_dtype)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Helper func for constructing a dataset object using the given options.\n",
    "\n",
    "def load_dataset(class_name='dataset.TFRecordDataset', data_dir=None, verbose=False, **kwargs):\n",
    "    adjusted_kwargs = dict(kwargs)\n",
    "    if 'tfrecord_dir' in adjusted_kwargs and data_dir is not None:\n",
    "        adjusted_kwargs['tfrecord_dir'] = os.path.join(data_dir, adjusted_kwargs['tfrecord_dir'])\n",
    "    if verbose:\n",
    "        print('Streaming data using %s...' % class_name)\n",
    "    dataset = tfutil.import_obj(class_name)(**adjusted_kwargs)\n",
    "    if verbose:\n",
    "        print('Dataset shape =', np.int32(dataset.shape).tolist())\n",
    "        print('Dynamic range =', dataset.dynamic_range)\n",
    "        print('Label size    =', dataset.label_size)\n",
    "    return dataset\n",
    "\n",
    "#----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def lerp(a, b, t): return a + (b - a) * t\n",
    "def lerp_clip(a, b, t): return a + (b - a) * tf.clip_by_value(t, 0.0, 1.0)\n",
    "def cset(cur_lambda, new_cond, new_lambda): return lambda: tf.cond(new_cond, new_lambda, cur_lambda)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Get/create weight tensor for a convolutional or fully-connected layer.\n",
    "\n",
    "def get_weight(shape, gain=np.sqrt(2), use_wscale=False, fan_in=None):\n",
    "    if fan_in is None: fan_in = np.prod(shape[:-1])\n",
    "    std = gain / np.sqrt(fan_in) # He init\n",
    "    if use_wscale:\n",
    "        wscale = tf.constant(np.float32(std), name='wscale')\n",
    "        return tf.get_variable('weight', shape=shape, initializer=tf.initializers.random_normal()) * wscale\n",
    "    else:\n",
    "        return tf.get_variable('weight', shape=shape, initializer=tf.initializers.random_normal(0, std))\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Fully-connected layer.\n",
    "\n",
    "def dense(x, fmaps, gain=np.sqrt(2), use_wscale=False):\n",
    "    if len(x.shape) > 2:\n",
    "        x = tf.reshape(x, [-1, np.prod([d.value for d in x.shape[1:]])])\n",
    "    w = get_weight([x.shape[1].value, fmaps], gain=gain, use_wscale=use_wscale)\n",
    "    w = tf.cast(w, x.dtype)\n",
    "    return tf.matmul(x, w)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Convolutional layer.\n",
    "\n",
    "def conv2d(x, fmaps, kernel, gain=np.sqrt(2), use_wscale=False):\n",
    "    assert kernel >= 1 and kernel % 2 == 1\n",
    "    w = get_weight([kernel, kernel, x.shape[1].value, fmaps], gain=gain, use_wscale=use_wscale)\n",
    "    w = tf.cast(w, x.dtype)\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME', data_format='NCHW')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Apply bias to the given activation tensor.\n",
    "\n",
    "def apply_bias(x):\n",
    "    b = tf.get_variable('bias', shape=[x.shape[1]], initializer=tf.initializers.zeros())\n",
    "    b = tf.cast(b, x.dtype)\n",
    "    if len(x.shape) == 2:\n",
    "        return x + b\n",
    "    else:\n",
    "        return x + tf.reshape(b, [1, -1, 1, 1])\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Leaky ReLU activation. Same as tf.nn.leaky_relu, but supports FP16.\n",
    "\n",
    "def leaky_relu(x, alpha=0.2):\n",
    "    with tf.name_scope('LeakyRelu'):\n",
    "        alpha = tf.constant(alpha, dtype=x.dtype, name='alpha')\n",
    "        return tf.maximum(x * alpha, x)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Nearest-neighbor upscaling layer.\n",
    "\n",
    "def upscale2d(x, factor=2):\n",
    "    assert isinstance(factor, int) and factor >= 1\n",
    "    if factor == 1: return x\n",
    "    with tf.variable_scope('Upscale2D'):\n",
    "        s = x.shape\n",
    "        x = tf.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
    "        x = tf.tile(x, [1, 1, 1, factor, 1, factor])\n",
    "        x = tf.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Fused upscale2d + conv2d.\n",
    "# Faster and uses less memory than performing the operations separately.\n",
    "\n",
    "def upscale2d_conv2d(x, fmaps, kernel, gain=np.sqrt(2), use_wscale=False):\n",
    "    assert kernel >= 1 and kernel % 2 == 1\n",
    "    w = get_weight([kernel, kernel, fmaps, x.shape[1].value], gain=gain, use_wscale=use_wscale, fan_in=(kernel**2)*x.shape[1].value)\n",
    "    w = tf.pad(w, [[1,1], [1,1], [0,0], [0,0]], mode='CONSTANT')\n",
    "    w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]])\n",
    "    w = tf.cast(w, x.dtype)\n",
    "    os = [tf.shape(x)[0], fmaps, x.shape[2] * 2, x.shape[3] * 2]\n",
    "    return tf.nn.conv2d_transpose(x, w, os, strides=[1,1,2,2], padding='SAME', data_format='NCHW')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Box filter downscaling layer.\n",
    "\n",
    "def downscale2d(x, factor=2):\n",
    "    assert isinstance(factor, int) and factor >= 1\n",
    "    if factor == 1: return x\n",
    "    with tf.variable_scope('Downscale2D'):\n",
    "        ksize = [1, 1, factor, factor]\n",
    "        return tf.nn.avg_pool(x, ksize=ksize, strides=ksize, padding='VALID', data_format='NCHW') # NOTE: requires tf_config['graph_options.place_pruned_graph'] = True\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Fused conv2d + downscale2d.\n",
    "# Faster and uses less memory than performing the operations separately.\n",
    "\n",
    "def conv2d_downscale2d(x, fmaps, kernel, gain=np.sqrt(2), use_wscale=False):\n",
    "    assert kernel >= 1 and kernel % 2 == 1\n",
    "    w = get_weight([kernel, kernel, x.shape[1].value, fmaps], gain=gain, use_wscale=use_wscale)\n",
    "    w = tf.pad(w, [[1,1], [1,1], [0,0], [0,0]], mode='CONSTANT')\n",
    "    w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]]) * 0.25\n",
    "    w = tf.cast(w, x.dtype)\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,2,2], padding='SAME', data_format='NCHW')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Pixelwise feature vector normalization.\n",
    "\n",
    "def pixel_norm(x, epsilon=1e-8):\n",
    "    with tf.variable_scope('PixelNorm'):\n",
    "        return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + epsilon)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Minibatch standard deviation.\n",
    "\n",
    "def minibatch_stddev_layer(x, group_size=4):\n",
    "    with tf.variable_scope('MinibatchStddev'):\n",
    "        group_size = tf.minimum(group_size, tf.shape(x)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
    "        s = x.shape                                             # [NCHW]  Input shape.\n",
    "        y = tf.reshape(x, [group_size, -1, s[1], s[2], s[3]])   # [GMCHW] Split minibatch into M groups of size G.\n",
    "        y = tf.cast(y, tf.float32)                              # [GMCHW] Cast to FP32.\n",
    "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMCHW] Subtract mean over group.\n",
    "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MCHW]  Calc variance over group.\n",
    "        y = tf.sqrt(y + 1e-8)                                   # [MCHW]  Calc stddev over group.\n",
    "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
    "        y = tf.cast(y, x.dtype)                                 # [M111]  Cast back to original data type.\n",
    "        y = tf.tile(y, [group_size, 1, s[2], s[3]])             # [N1HW]  Replicate over group and pixels.\n",
    "        return tf.concat([x, y], axis=1)                        # [NCHW]  Append as new fmap.\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Generator network used in the paper.\n",
    "\n",
    "def G_paper(\n",
    "    latents_in,                         # First input: Latent vectors [minibatch, latent_size].\n",
    "    labels_in,                          # Second input: Labels [minibatch, label_size].\n",
    "    num_channels        = 1,            # Number of output color channels. Overridden based on dataset.\n",
    "    resolution          = 32,           # Output resolution. Overridden based on dataset.\n",
    "    label_size          = 0,            # Dimensionality of the labels, 0 if no labels. Overridden based on dataset.\n",
    "    fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
    "    fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
    "    fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
    "    latent_size         = None,         # Dimensionality of the latent vectors. None = min(fmap_base, fmap_max).\n",
    "    normalize_latents   = True,         # Normalize latent vectors before feeding them to the network?\n",
    "    use_wscale          = True,         # Enable equalized learning rate?\n",
    "    use_pixelnorm       = True,         # Enable pixelwise feature vector normalization?\n",
    "    pixelnorm_epsilon   = 1e-8,         # Constant epsilon for pixelwise feature vector normalization.\n",
    "    use_leakyrelu       = True,         # True = leaky ReLU, False = ReLU.\n",
    "    dtype               = 'float32',    # Data type to use for activations and outputs.\n",
    "    fused_scale         = True,         # True = use fused upscale2d + conv2d, False = separate upscale2d layers.\n",
    "    structure           = None,         # 'linear' = human-readable, 'recursive' = efficient, None = select automatically.\n",
    "    is_template_graph   = False,        # True = template graph constructed by the Network class, False = actual evaluation.\n",
    "    **kwargs):                          # Ignore unrecognized keyword args.\n",
    "    \n",
    "    resolution_log2 = int(np.log2(resolution))\n",
    "    assert resolution == 2**resolution_log2 and resolution >= 4\n",
    "    def nf(stage): return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "    def PN(x): return pixel_norm(x, epsilon=pixelnorm_epsilon) if use_pixelnorm else x\n",
    "    if latent_size is None: latent_size = nf(0)\n",
    "    if structure is None: structure = 'linear' if is_template_graph else 'recursive'\n",
    "    act = leaky_relu if use_leakyrelu else tf.nn.relu\n",
    "    \n",
    "    latents_in.set_shape([None, latent_size])\n",
    "    labels_in.set_shape([None, label_size])\n",
    "    combo_in = tf.cast(tf.concat([latents_in, labels_in], axis=1), dtype)\n",
    "    lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0.0), trainable=False), dtype)\n",
    "\n",
    "    # Building blocks.\n",
    "    def block(x, res): # res = 2..resolution_log2\n",
    "        with tf.variable_scope('%dx%d' % (2**res, 2**res)):\n",
    "            if res == 2: # 4x4\n",
    "                if normalize_latents: x = pixel_norm(x, epsilon=pixelnorm_epsilon)\n",
    "                with tf.variable_scope('Dense'):\n",
    "                    x = dense(x, fmaps=nf(res-1)*16, gain=np.sqrt(2)/4, use_wscale=use_wscale) # override gain to match the original Theano implementation\n",
    "                    x = tf.reshape(x, [-1, nf(res-1), 4, 4])\n",
    "                    x = PN(act(apply_bias(x)))\n",
    "                with tf.variable_scope('Conv'):\n",
    "                    x = PN(act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, use_wscale=use_wscale))))\n",
    "            else: # 8x8 and up\n",
    "                if fused_scale:\n",
    "                    with tf.variable_scope('Conv0_up'):\n",
    "                        x = PN(act(apply_bias(upscale2d_conv2d(x, fmaps=nf(res-1), kernel=3, use_wscale=use_wscale))))\n",
    "                else:\n",
    "                    x = upscale2d(x)\n",
    "                    with tf.variable_scope('Conv0'):\n",
    "                        x = PN(act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, use_wscale=use_wscale))))\n",
    "                with tf.variable_scope('Conv1'):\n",
    "                    x = PN(act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, use_wscale=use_wscale))))\n",
    "            return x\n",
    "    def toimage(x, res): # res = 2..resolution_log2\n",
    "        lod = resolution_log2 - res\n",
    "        with tf.variable_scope('ToImage_lod%d' % lod):\n",
    "            return apply_bias(conv2d(x, fmaps=num_channels, kernel=1, gain=1, use_wscale=use_wscale))\n",
    "\n",
    "    # Linear structure: simple but inefficient.\n",
    "    if structure == 'linear':\n",
    "        x = block(combo_in, 2)\n",
    "        images_out = toimage(x, 2)\n",
    "        for res in range(3, resolution_log2 + 1):\n",
    "            lod = resolution_log2 - res\n",
    "            x = block(x, res)\n",
    "            img = toimage(x, res)\n",
    "            images_out = upscale2d(images_out)\n",
    "            with tf.variable_scope('Grow_lod%d' % lod):\n",
    "                images_out = lerp_clip(img, images_out, lod_in - lod)\n",
    "\n",
    "    # Recursive structure: complex but efficient.\n",
    "    if structure == 'recursive':\n",
    "        def grow(x, res, lod):\n",
    "            y = block(x, res)\n",
    "            img = lambda: upscale2d(toimage(y, res), 2**lod)\n",
    "            if res > 2: img = cset(img, (lod_in > lod), lambda: upscale2d(lerp(toimage(y, res), upscale2d(toimage(x, res - 1)), lod_in - lod), 2**lod))\n",
    "            if lod > 0: img = cset(img, (lod_in < lod), lambda: grow(y, res + 1, lod - 1))\n",
    "            return img()\n",
    "        images_out = grow(combo_in, 2, resolution_log2 - 2)\n",
    "        \n",
    "    assert images_out.dtype == tf.as_dtype(dtype)\n",
    "    images_out = tf.identity(images_out, name='images_out')\n",
    "    return images_out\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Discriminator network used in the paper.\n",
    "\n",
    "def D_paper(\n",
    "    images_in,                          # Input: Images [minibatch, channel, height, width].\n",
    "    num_channels        = 1,            # Number of input color channels. Overridden based on dataset.\n",
    "    resolution          = 32,           # Input resolution. Overridden based on dataset.\n",
    "    label_size          = 0,            # Dimensionality of the labels, 0 if no labels. Overridden based on dataset.\n",
    "    fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
    "    fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
    "    fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
    "    use_wscale          = True,         # Enable equalized learning rate?\n",
    "    mbstd_group_size    = 4,            # Group size for the minibatch standard deviation layer, 0 = disable.\n",
    "    dtype               = 'float32',    # Data type to use for activations and outputs.\n",
    "    fused_scale         = True,         # True = use fused conv2d + downscale2d, False = separate downscale2d layers.\n",
    "    structure           = None,         # 'linear' = human-readable, 'recursive' = efficient, None = select automatically\n",
    "    is_template_graph   = False,        # True = template graph constructed by the Network class, False = actual evaluation.\n",
    "    **kwargs):                          # Ignore unrecognized keyword args.\n",
    "    \n",
    "    resolution_log2 = int(np.log2(resolution))\n",
    "    assert resolution == 2**resolution_log2 and resolution >= 4\n",
    "    def nf(stage): return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "    if structure is None: structure = 'linear' if is_template_graph else 'recursive'\n",
    "    act = leaky_relu\n",
    "\n",
    "    images_in.set_shape([None, num_channels, resolution, resolution])\n",
    "    images_in = tf.cast(images_in, dtype)\n",
    "    lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0.0), trainable=False), dtype)\n",
    "\n",
    "    # Building blocks.\n",
    "    def fromimage(x, res): # res = 2..resolution_log2\n",
    "        with tf.variable_scope('FromImage_lod%d' % (resolution_log2 - res)):\n",
    "            return act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=1, use_wscale=use_wscale)))\n",
    "    def block(x, res): # res = 2..resolution_log2\n",
    "        with tf.variable_scope('%dx%d' % (2**res, 2**res)):\n",
    "            if res >= 3: # 8x8 and up\n",
    "                with tf.variable_scope('Conv0'):\n",
    "                    x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, use_wscale=use_wscale)))\n",
    "                if fused_scale:\n",
    "                    with tf.variable_scope('Conv1_down'):\n",
    "                        x = act(apply_bias(conv2d_downscale2d(x, fmaps=nf(res-2), kernel=3, use_wscale=use_wscale)))\n",
    "                else:\n",
    "                    with tf.variable_scope('Conv1'):\n",
    "                        x = act(apply_bias(conv2d(x, fmaps=nf(res-2), kernel=3, use_wscale=use_wscale)))\n",
    "                    x = downscale2d(x)\n",
    "            else: # 4x4\n",
    "                if mbstd_group_size > 1:\n",
    "                    x = minibatch_stddev_layer(x, mbstd_group_size)\n",
    "                with tf.variable_scope('Conv'):\n",
    "                    x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, use_wscale=use_wscale)))\n",
    "                with tf.variable_scope('Dense0'):\n",
    "                    x = act(apply_bias(dense(x, fmaps=nf(res-2), use_wscale=use_wscale)))\n",
    "                with tf.variable_scope('Dense1'):\n",
    "                    x = apply_bias(dense(x, fmaps=1+label_size, gain=1, use_wscale=use_wscale))\n",
    "            return x\n",
    "    \n",
    "    # Linear structure: simple but inefficient.\n",
    "    if structure == 'linear':\n",
    "        img = images_in\n",
    "        x = fromimage(img, resolution_log2)\n",
    "        for res in range(resolution_log2, 2, -1):\n",
    "            lod = resolution_log2 - res\n",
    "            x = block(x, res)\n",
    "            img = downscale2d(img)\n",
    "            y = fromimage(img, res - 1)\n",
    "            with tf.variable_scope('Grow_lod%d' % lod):\n",
    "                x = lerp_clip(x, y, lod_in - lod)\n",
    "        combo_out = block(x, 2)\n",
    "\n",
    "    # Recursive structure: complex but efficient.\n",
    "    if structure == 'recursive':\n",
    "        def grow(res, lod):\n",
    "            x = lambda: fromimage(downscale2d(images_in, 2**lod), res)\n",
    "            if lod > 0: x = cset(x, (lod_in < lod), lambda: grow(res + 1, lod - 1))\n",
    "            x = block(x(), res); y = lambda: x\n",
    "            if res > 2: y = cset(y, (lod_in > lod), lambda: lerp(x, fromimage(downscale2d(images_in, 2**(lod+1)), res - 1), lod_in - lod))\n",
    "            return y()\n",
    "        combo_out = grow(2, resolution_log2 - 2)\n",
    "\n",
    "    assert combo_out.dtype == tf.as_dtype(dtype)\n",
    "    scores_out = tf.identity(combo_out[:, :1], name='scores_out')\n",
    "    labels_out = tf.identity(combo_out[:, 1:], name='labels_out')\n",
    "    return scores_out, labels_out\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5bef0a826508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m         'variables':        vars}\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mtfutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork_import_handlers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatch_theano_gan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m#----------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfutil' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
